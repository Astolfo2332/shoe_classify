from utils.model_generator import get_model
import torch
import json
import os
import nibabel as nib
import numpy as np
from utils.normalizaci√≥n import normalice_and_save, search_paths
from torch.utils.data import DataLoader

class InferenceData:
    def __init__(self, file_paths):
        self.file_paths = file_paths

    def __len__(self):
        return len(self.file_paths)

    def __getitem__(self, idx):
        image_path = self.file_paths[idx]
        image = nib.load(image_path).get_fdata()
        image = image.astype(np.float32)
        image = np.expand_dims(image, axis=0)
        image = torch.from_numpy(image)
        return image, image_path



def inference_data(inference_path):

    normalice_and_save(inference_path)
    files = search_paths(inference_path, "brain_normalized.nii.gz")

    weights_path = 'models/Test_4_New_Metrics/custom_resnet152/Adam/150/best_model_custom_resnet152.pth'
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

    weights = torch.load(weights_path)
    model = get_model('custom_resnet152').to(device)
    model.load_state_dict(weights)
    inference_dict = {}
    inference_data = InferenceData(files)
    inference_data = DataLoader(inference_data, batch_size=len(files), shuffle=False)


    for image, path in inference_data:
        model.eval()
        with torch.inference_mode():

            images = image.to(device)
            output = model(images)
            logits = torch.softmax(output, dim=1)
            inferences = torch.argmax(logits, dim=1)

            for path, logit, inference in zip(path, logits, inferences):
                file_name = path.split("/")[-1]
                inference_dict[file_name] = {
                    "logits": logit.cpu().detach().numpy().tolist(),
                    "inference": inference.cpu().detach().numpy().tolist()
                }

    with open('inference/labels.json', 'w',encoding="utf") as f:
        json.dump(inference_dict, f, indent=4)